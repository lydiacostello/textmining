{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Implement the textbook's system for supervised learning, modifying as needed to run on our literary corpus. Classify the novels as authored by either British/American or female/male writers. Assess the accuracy of the results.\n",
    "\n",
    "As an optional challenge, see what you can do to speed up the system and/or to get better classification accuracy. In general, this will mean using different classifiers and/or other methods in the pipeline, though it could involve some additional feature engineering as well.\n",
    "\n",
    "You can find the textbook's code via its [GitHub repo](https://github.com/foxbook/atap). You're free to copy what you need, but be aware that you'll need to make some changes to work with our data (not least, to use our corpus readers from problem sets 3/4). As ever, make sur eyou understand what the supplied code is doing. You can only modifiy it if you have a strong sense of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "class CorpusLoader(object):\n",
    "\n",
    "    def __init__(self, reader, folds=12, shuffle=True, categories=None):\n",
    "        self.reader = reader\n",
    "        self.folds  = KFold(n_splits=folds, shuffle=shuffle)\n",
    "        self.files  = np.asarray(self.reader.fileids(categories=categories))\n",
    "\n",
    "    def fileids(self, idx=None):\n",
    "        if idx is None:\n",
    "            return self.files\n",
    "        return self.files[idx]\n",
    "\n",
    "    def documents(self, idx=None):\n",
    "        for fileid in self.fileids(idx):\n",
    "            yield list(self.reader.docs(fileids=[fileid]))\n",
    "\n",
    "    def labels(self, idx=None):\n",
    "        return [\n",
    "            self.reader.categories(fileids=[fileid])[0]\n",
    "            for fileid in self.fileids(idx)\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for train_index, test_index in self.folds.split(self.files):\n",
    "            X_train = self.documents(train_index)\n",
    "            y_train = self.labels(train_index)\n",
    "\n",
    "            X_test = self.documents(test_index)\n",
    "            y_test = self.labels(test_index)\n",
    "\n",
    "            yield X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "DOC_PATTERN = r'.+\\.txt'        # Documents are just files that end in '.txt'\n",
    "PKL_PATTERN = r'.+\\.pickle'     # Pickled files end in .pickle\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*' # We won't use this, but fall back to directory-based labels\n",
    "                                # if no other labels are supplied\n",
    "\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "from   glob import glob\n",
    "from   nltk.corpus.reader.api import CorpusReader\n",
    "from   nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "from   nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "def make_cat_map(path, extension):\n",
    "    \"\"\"\n",
    "    Takes a directory path and file extension (e.g., 'txt').\n",
    "    Returns a dictionary of file:category mappings from standard file names:\n",
    "      nation-author-title-year-gender\n",
    "    \"\"\"\n",
    "    file_paths = glob(os.path.join(path, f'*.{extension}'))\n",
    "    file_names = [os.path.split(i)[1] for i in file_paths]\n",
    "    category_map = {} # Dict to hold filename:[categories] mappings\n",
    "    for file in file_names:\n",
    "        parsed = file.rstrip(f'.{extension}').split('-') # strip extension and split on hyphens\n",
    "        nation = parsed[0]\n",
    "        gender = parsed[4]\n",
    "        category_map[file] = [nation, gender, nation+gender]\n",
    "    return category_map\n",
    "\n",
    "class TMNCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for categorized text documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        fileids=DOC_PATTERN,\n",
    "        encoding='utf8', \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            # First, try to build a cat_map from standard-style filenames\n",
    "            try: \n",
    "                kwargs['cat_map'] = make_cat_map(root, 'txt')\n",
    "            # On error, fall back to dir names for categories    \n",
    "            except Exception as e:\n",
    "                print(type(e), e, \"\\nUnable to build category map from file names.\\nFalling back to categories by directory name.\")\n",
    "                kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        \n",
    "    def resolve(self, fileids, categories):\n",
    "            \"\"\"\n",
    "            Returns a list of fileids or categories depending on what is passed\n",
    "            to each internal corpus reader function. Implemented similarly to\n",
    "            the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "            \"\"\"\n",
    "            if fileids is not None and categories is not None:\n",
    "                raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "            if categories is not None:\n",
    "                return self.fileids(categories)\n",
    "            return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of a document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "\n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "            \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses splitlines() to parse the paragraphs from plain text.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for doc in self.docs(fileids):\n",
    "            for par in doc.splitlines():\n",
    "                if len(par) > 0:\n",
    "                    yield par\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in sentence tokenizer to extract sentences from the\n",
    "        paragraphs. Note that this method uses BeautifulSoup to parse HTML.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in sent_tokenize(paragraph):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in word tokenizer to extract tokens from sentences.\n",
    "        Note that this method uses BeautifulSoup to parse HTML content.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for sentence in self.sents(fileids):\n",
    "            for token in wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "\n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.time()\n",
    "\n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "\n",
    "            for sent in sent_tokenize(para):\n",
    "                counts['sents'] += 1\n",
    "\n",
    "                for word in wordpunct_tokenize(sent):\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "\n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self.resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files':  n_fileids,\n",
    "            'categories': n_topics,\n",
    "            'paragraphs':  counts['paras'],\n",
    "            'sentences':  counts['sents'],\n",
    "            'words':  counts['words'],\n",
    "            'vocabulary_size':  len(tokens),\n",
    "            'lexical_diversity': float(counts['words']) / float(len(tokens)),\n",
    "            'paras_per_doc':  float(counts['paras']) / float(n_fileids),\n",
    "            'words_per_doc':  float(counts['words']) / float(n_fileids),\n",
    "            'sents_per_para':  float(counts['sents']) / float(counts['paras']),\n",
    "            'secs':   time.time() - started,\n",
    "        }\n",
    "    \n",
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            # First, try to build a cat_map from standard-style filenames\n",
    "            try: \n",
    "                kwargs['cat_map'] = make_cat_map(root, 'pickle')\n",
    "            # On error, fall back to dir names for categories    \n",
    "            except Exception as e:\n",
    "                print(type(e), e, \"\\nUnable to build category map from file names.\\nFalling back to categories by directory name.\")\n",
    "                kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids, categories):\n",
    "            yield token[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.normalize(document[0])\n",
    "\n",
    "\n",
    "def create_pipeline(estimator, reduction=False):\n",
    "\n",
    "    steps = [\n",
    "        ('normalize', TextNormalizer()),\n",
    "        ('vectorize', TfidfVectorizer(\n",
    "            tokenizer=identity, preprocessor=None, lowercase=False\n",
    "        ))\n",
    "    ]\n",
    "\n",
    "    if reduction:\n",
    "        steps.append((\n",
    "            'reduction', TruncatedSVD(n_components=10000)\n",
    "        ))\n",
    "\n",
    "    # Add the estimator\n",
    "    steps.append(('classifier', estimator))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "labels = [\"M\",\"F\"]\n",
    "reader = PickledCorpusReader('../data/pickled')\n",
    "loader = CorpusLoader(reader, 5, shuffle=True, categories=labels)\n",
    "\n",
    "models = []\n",
    "for form in (LogisticRegression, SGDClassifier):\n",
    "    models.append(create_pipeline(form(), True))\n",
    "    models.append(create_pipeline(form(), False))\n",
    "\n",
    "models.append(create_pipeline(MultinomialNB(), False))\n",
    "models.append(create_pipeline(GaussianNB(), True))\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def score_models(models, loader):\n",
    "    for model in models:\n",
    "\n",
    "        name = model.named_steps['classifier'].__class__.__name__\n",
    "        if 'reduction' in model.named_steps:\n",
    "            name += \" (TruncatedSVD)\"\n",
    "\n",
    "        scores = {\n",
    "            'model': str(model),\n",
    "            'name': name,\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'time': [],\n",
    "        }\n",
    "\n",
    "        for X_train, X_test, y_train, y_test in loader:\n",
    "            start = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            scores['time'].append(time.time() - start)\n",
    "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "            scores['precision'].append(precision_score(y_test, y_pred, average='weighted'))\n",
    "            scores['recall'].append(recall_score(y_test, y_pred, average='weighted'))\n",
    "            scores['f1'].append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "        yield scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for scores in score_models(models, loader):\n",
    "        with open('results.json', 'a') as f:\n",
    "            f.write(json.dumps(scores) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Offer a brief set of observations on the system you've built. Does it perform well? How so? How not? What would you try to change in order to make it better (for whatever definitions of \"better\" seem appropriate to you)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
