{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and unsupervised learning\n",
    "\n",
    "The simplest version of your task is to produce two representations of the 40-volume course corpus:\n",
    "\n",
    "1. A plausible topic model using latent Dirichlet allocation\n",
    "1. A _k_-means clustering on the basis of (normalized) word frequencies\n",
    "\n",
    "The _k_-means clustering should take the form of cluster assignments per volume, as illustrated in the textbook. The topic model should be represented as a list of keywords associated with each topic, again as shown in the textbook.\n",
    "\n",
    "There is at least one thing you'll need to do in addition to the textbook examples:\n",
    "\n",
    "* Use the topic model output (or any other method) to uncover and remove additional proper names from your data.\n",
    "\n",
    "There are a bunch of proper names in the corpus. As we discussed in class, most of these are undesirable in the sense that they do not represent meaningful semantic (that is, subject-matter) connections between any two volumes that happen to share them. So we want to get rid of them. \n",
    "\n",
    "One way to do this is to build topic models repreatedly, adding names from each output run to your stopword list until you stop seeing personal names among the top keywords. Another is to remove all proper nouns or named entities of type \"person\" from the input data. It's up to you how you approach the problem.\n",
    "\n",
    "To help you get started, I've supplied three large stopword lists (from Jockers, Underwood, and Goldstone) in the `'data/wordlists'` directory on GitHub. Each file name begins with 'stopwords-'. I've also given you a function to import those lists and add their content to the basic NLTK English stopwords list. You can manage your own stopword list either in your code or (better yet) as an additional stopword file that you load alongside the others.\n",
    "\n",
    "You'll notice, too, that I've supplied a short list (just one item for now) of offensive terms that should not be removed from the corpus (because they are meaningful), but also should not be displayed in raw form in the output. The supplied `mask_offensive()` function will return a version of these words with all but the first and last two letters replaced by `'*'`. I've supplied a lightly modified version of the `normalize()` function that uses `mask_offensive()` to apply this transformation. \n",
    "\n",
    "## Your minimal outputs should be:\n",
    "\n",
    "1. Topic keywords for each topic in the final version of your model, showing no (or very few) proper names in any topic.\n",
    "1. Cluster assignments from a _k_-means clustering on the data with names and other stopwords removed.\n",
    "1. A (very) brief discussion of your results, emphasizing your sense of how well they reflect any knowledge you may have about the books in question.\n",
    "\n",
    "Most of the two primary tasks can be accomplished with minor modification to the code included in chapter 6 of the textbook.\n",
    "\n",
    "## Optional ways to extend this work\n",
    "\n",
    "If you have the time and inclination to push yourself:\n",
    "\n",
    "* Visualize the _k_-means output in two dimensions, coloring each volume by the cluster to which it is assigned. You might want to consult the vectorization problem set answers for a model approach to this type of visualization.\n",
    "* Visualize the output of your topic model using `pyLDAvis`.\n",
    "* Repeat the clustering using topic fractions per document in place of word frequencies. To do this, you'll need to produce a doc-topic matrix to substitute for the doc-term matrix. You can do this by running the `.transform()` method of your trained topic model over the normalized corpus.\n",
    "* Explore a range of settings for normalization, vectorization, dimension-reduction, and modeling parameters to get a feel for how they affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF, PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from collections import Counter\n",
    "\n",
    "# Where are the corpus texts on your system\n",
    "pickle_dir = os.path.join('..', 'data', 'pickled')\n",
    "wordlist_dir = os.path.join('..', 'data', 'wordlists')\n",
    "\n",
    "# Import our libraries\n",
    "sys.path.append(os.path.join('..', 'libraries'))\n",
    "from TMN import PickledCorpusReader\n",
    "\n",
    "def get_wordlists_from_files(wordlist_dir):\n",
    "    \"\"\"Read stopwords and offensive terms from files in wordlist_dir, return a set for each\"\"\"\n",
    "    stopwords = []\n",
    "    offensive = []\n",
    "    stopword_files = glob.glob(wordlist_dir+'/stop*.txt')\n",
    "    offensive_files = glob.glob(wordlist_dir+'/offensive*.txt')\n",
    "    for f in stopword_files:\n",
    "        with open(f, 'r') as fh:\n",
    "            for line in fh.readlines():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    stopwords.append(line)\n",
    "    for f in offensive_files:\n",
    "        with open(f, 'r') as fh:\n",
    "            for line in fh.readlines():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    offensive.append(line)\n",
    "    return set(stopwords), set(offensive)\n",
    "\n",
    "STOPWORDS, OFFENSIVE = get_wordlists_from_files(wordlist_dir)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english')).union(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "\n",
    "def mask_offensive(token):\n",
    "    \"\"\"Make lemmatized offensive words less objectionable for display\"\"\"\n",
    "    if token in OFFENSIVE:\n",
    "        return token[0]+(len(token)-3)*'*'+token[-2:]\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            mask_offensive(self.lemmatize(token, tag).lower())\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "    TEXTBOOK CODE HERE. \n",
    "    I have supplied only the modified normalize() method that masks offensive terms.\n",
    "    You need to get the rest of the code for this class and for the SklearnTopicModels\n",
    "    class from the textbook, or else write it yourself.\n",
    "    \n",
    "    NB. To remove proper nouns, you can modify the normalize() function pretty trivially.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _k_-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def is_punct(token):\n",
    "    # Is every character punctuation?\n",
    "    return all(\n",
    "        unicodedata.category(char).startswith('P')\n",
    "        for char in token\n",
    "    )\n",
    "\n",
    "\n",
    "def wnpos(tag):\n",
    "    # Return the WordNet POS tag from the Penn Treebank tag\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "\n",
    "def normalize(document, stopwords=STOPWORDS):\n",
    "    \"\"\"\n",
    "    Takes a document = list of (token, pos_tag) tuples\n",
    "    Removes stopwords and punctuation, lowercases, lemmatizes\n",
    "    \"\"\"\n",
    "\n",
    "    for token, tag in document:\n",
    "        token = token.lower().strip()\n",
    "\n",
    "        if is_punct(token) or (token in stopwords):\n",
    "            continue\n",
    "            \n",
    "        yield mask_offensive(lemmatizer.lemmatize(token, wnpos(tag)))\n",
    "\n",
    "\"\"\"\n",
    "TEXTBOOK/YOUR CODE HERE.\n",
    "Pretty much just the KMeansTopics class; the rest is included above, again modifying the\n",
    "normalize() function to include output masking.\n",
    "NB. You'll need to remove names in the same way you did for the topic model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "A (very) brief discussion of your results, emphasizing your sense of how well they reflect any knowledge you may have about the books in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
